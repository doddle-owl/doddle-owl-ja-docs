オントロジー構築モジュール
================================

.. contents:: コンテンツ 
   :depth: 3

オントロジー構築モジュールは，階層構築モジュールおよび関係構築モジュールから構成される．階層構築モジュールでは，参照オントロジーの概念階層を参照し，概念階層初期モデルを構築する（階層構築）．関係構築モジュールでは，入力文書および入力概念集合から，共起性に基づく手法により概念対集合を獲得する（関係構築）．概念階層初期モデルおよび概念対集合は，初期領域オントロジーであり，オントロジー洗練モジュールにおいてユーザインタラクションを通して洗練される．

以下では，階層構築モジュールおよび関係構築モジュールについて説明する．

階層構築モジュール
------------------

階層構築モジュールでは，参照オントロジーの概念階層を参照し，領域オントロジーの基礎となる概念階層初期モデルを構築する．入力モジュールにおいて，入力語と完全照合した入力概念（完全照合概念）と部分照合した入力概念（部分照合概念）により，階層構築方法が異なる．以下では，完全照合概念と部分照合概念のそれぞれについて，階層構築方法を説明する．


完全照合概念の階層構築
~~~~~~~~~~~~~~~~~~~~~~

.. _process_of_perfectly_matched:
.. figure:: figures/process_of_perfectly_matched_concept_tree_construction.png
   :scale: 80 %
   :alt: 完全照合概念の階層構築工程
   :align: center

   完全照合概念の階層構築工程

:numref:`process_of_perfectly_matched` に完全照合概念の階層構築工程を示す．はじめに，参照オントロジーから，入力モジュールにより獲得した完全照合概念を末端ノードとするルート概念までのパスを抽出し，合成する．これをベストマッチモデルと呼ぶ．

:numref:`process_of_perfectly_matched` のベストマッチモデルは，1 重線で囲まれたノードである入力概念ノード，2 重線で囲まれたノードであるSIN (a Salient Internal Nodes)，点線で囲まれたノードである不要中間ノードの3 種類のノードから構成される．入力概念ノードは，ユーザが選択した入力語に対応する参照オントロジー中の概念であり，領域にとって必須である．参照オントロジーから抽出したノードのうち，入力概念ノード以外のノードはSIN または不要中間ノードとなる．SIN は，入力概念ノードを一つ以上子ノードとして持つノードである．SIN は，各入力概念間の位相関係（祖先・親子・兄弟関係）を保持することに貢献する．一方，不要中間ノードは，入力概念ノードを子ノードとして持たないノードである．不要中間ノードはSIN とは異なり，各入力概念間の位相関係を保持することに貢献しないため，階層構築モジュールは階層構築において不要な概念であると見なし，ベストマッチモデルから削除する．不要中間ノードを削除する工程を剪定と呼ぶ．剪定によって得られた入力概念ノードとSIN のみから構成される概念階層を概念階層初期モデルと呼ぶ．概念階層初期モデルは， **概念階層洗練手法** を用いて，ユーザとのインタラクションにより洗練され，最終的な領域オントロジーにおける概念階層となる．

部分照合概念の階層構築
~~~~~~~~~~~~~~~~~~~~~~

.. _process_of_partially_matched:
.. figure:: figures/process_of_partially_matched_concept_tree_construction.png
   :scale: 80 %
   :alt: 部分照合概念の階層構築工程
   :align: center

   部分照合概念の階層構築工程

階層構築モジュールでは，部分照合概念について語尾および語頭による階層化を行う．図2 に部分照合概念の階層構築例を示す．ここで，部分照合概念とは，参照オントロジー中の概念の見出しと部分的に照合する入力語を概念化したものである．入力概念選択モジュールで説明したように，入力語が完全照合しなかった場合，入力語を形態素解析し，語尾を含むように部分照合を行っている．ここで，部分照合概念の見出しについて，語尾を含むように照合された部分を語尾部分，それ以前の部分を語頭部分と呼ぶことにする．例えば，「ゲージ情報」という入力語が参照オントロジー中の「情報」概念と部分照合した場合，「ゲージ」を語頭部分，「情報」を語尾部分と呼ぶ．また，入力概念選択モジュールにおいて，ユーザは部分照合した入力語を照合した概念の別見出しとするか，下位概念とするかを選択する．ここでは，下位概念とするほうをユーザが選択したものとして説明する．

:numref:`process_of_partially_matched` では，はじめに，ユーザは，入力語として「ゲージ」，「レーダー」，「ゲージ情報」，「レーダー情報」，「モデル情報」を選択した．「ゲージ」および「レーダー」は，参照オントロジー中にそれらを見出しとする概念が存在するため，図1に示した完全照合概念の階層構築工程に従って階層構築される．「ゲージ情報」，「レーダー情報」，「モデル情報」は，参照オントロジー中の「情報」概念と部分照合した．語尾による階層化により，はじめに，「情報」概念が完全照合概念の階層構築工程に従って階層構築され，次に，「ゲージ情報」，「レーダー情報」，「モデル情報」が概念化され，「情報」概念の下位概念として定義される．さらに，語頭による階層化では，部分照合概念の語頭部分に着目し，語頭部分を見出しとして持つ概念が構築中の概念階層内に存在する場合，その概念の上位概念と部分照合概念の語尾部分と照合した概念の見出しを組み合わせた見出しを持つ概念を新たに作成する．語頭部分が照合した部分照合概念は，新たに作成された概念の下位概念として階層関係が再定義される．部分照合概念の語頭部分は，部分照合概念を修飾していることが多い．そのため，語頭による階層化により，語尾による階層化のみに比べて，より詳細な階層構築を行うことができると考えられる．

:numref:`process_of_partially_matched` の語尾による階層化により構築された概念階層では，部分照合概念である「ゲージ情報」概念および「レーダー情報」概念の語頭部分にあたる「ゲージ」および「レーダー」を見出しとして持つ，「ゲージ」概念および「レーダー」概念が「計器」概念の下位概念として定義されている．ここで，語頭による階層化により，「計器」概念と「情報」概念を組み合わせた「計器情報」概念が新規に作成され，「ゲージ情報」概念および「レーダー情報」概念の上位概念として，階層化が行われる．「計器情報」概念を定義することにより，「モデル情報」概念と「ゲージ情報」概念および「レーダー情報」概念という計器に関する情報を分類することができる．

関係構築モジュール
------------------

その他の関係の定義を支援するために，関係構築モジュールでは，WordSpace と相関ルールの二つの共起性に基づく手法を用いて，入力文書および入力語彙からその他の関係の候補となる概念対を獲得する．

WordSpace による概念対の抽出
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

共起統計の計算手法としてWordSpace [Hearst96]_ を利用する．WordSpace とは，語彙の共起統計から大規模な単語群の意味表現を誘導するコーパスに基づく方法である．WordSpaceによって，出現語句を共起情報を含むベクトルとして表現できる．この単語ベクトルの集合である多次元ベクトル空間がWordSpace であり，2 ベクトル間の内積は出現語句の文脈類似度の指標となる．WordSpace から得られる共起情報を基に，文脈類似概念対を入力文書から獲得し，その他の関係定義に関わる可能性のある概念対として利用する．“文脈の類似は，その語句間の何らかの概念関係の存在を示唆している” と仮定する．　

以下では，WordSpace に基づく文脈類似概念対の獲得手順（ :numref:`wordspace` ）について説明する．

.. _wordspace:
.. figure:: figures/extraction_of_related_concept_pairs_using_WordSpace.png
   :scale: 80 %
   :alt: 文脈類似概念対の獲得手順
   :align: center

   文脈類似概念対の獲得手順

1. 高頻度単語N-gram の抽出
""""""""""""""""""""""""""

専門文書中からN 個の単語から構成される句（単語N-gram）を抽出し，共起の最小単位として用いる．文字単位のN-gram 統計を取るのに比べ意味の無い文字列の共起情報を除外でき，より専門文書の文脈表現に役立つ情報が抽出できる．この際抽出される句は，標準形に変換し，同形のものをまとめることで重複を排除している．ここで抽出された単語N-gram 集合の中から，専門文書における出現頻度の高い単語N-gram（高頻度単語N-gram）をWordSpace の構築に用いる．これにより入力文書は高頻度単語N-gram の配列とみなせる．関係構築モジュールでは，高頻度単語N-gram を抽出する際に，単語N-gram の単語数N および出現数をユーザは設定することができる．

.. note::
    [Hearst96]_ においては文字単位の共起を用いてWordSpace の構築を行っているが，関係構築モジュールでは単語単位N-gram の共起を最小単位として扱う．従って，通常のWordSpace 構築時に文字単位共起をある程度まとまった形で表現するために行う4-gram ベクトル構築工程は行わない．

2. 文脈ベクトルの構築
"""""""""""""""""""""

次に，ある二つの入力語の文脈を比較するために，文脈ベクトル(context vector)を構築する．文脈ベクトルとは，ある入力語周辺の高頻度単語N-gram の出現回数をベクトルで表現したものである．文脈ベクトル :math:`\overrightarrow{w_i}` の要素 :math:`a_{i,j}` は，入力語 :math:`w_i` の出現場所周辺（ **文脈スコープ** ）の高頻度単語N-gram :math:`g_j` の出現回数である．関係構築モジュールでは，文脈スコープとして，入力語 :math:`w_i` の前後何語以内に含まれる高頻度単語N-gram を文脈ベクトルの構築に用いるかをユーザは設定することができる．

3. 入力語ベクトルの構築
"""""""""""""""""""""""

次に，文脈ベクトルから入力語のベクトル表現である **入力語ベクトル(input term vector)**  を導く．入力語ベクトル :math:`\overrightarrow{W_i}` は，専門文書において，入力語 :math:`w_i` の全出現場所についての文脈ベクトル :math:`\overrightarrow{w_i}` の和によって表される．

4. 概念ベクトルの構築
"""""""""""""""""""""

次に，入力語ベクトルから入力概念のベクトル表現である **概念ベクトル(concept vector)** を導く．入力概念選択モジュールによって，入力語に対応する参照オントロジー中の概念（入力概念）は特定されている．入力概念の見出し（入力語）における入力語ベクトルの和が概念ベクトルとなる．概念ベクトル :math:`\overrightarrow{C}` は， :eq:`concept_vector` で表される． :math:`\mathcal{A}(w)` は，入力語 :math:`w` の専門文書における全出現場所を表す．:math:`\overrightarrow{w}(i)` は，入力語 :math:`w` の専門文書中の位置 :math:`i` における文脈ベクトルを表す．:math:`synset(C)` は，概念 における見出し集合を表す．

.. math:: \overrightarrow{C} & = & \sum_{w \in {synset(C)}} (\sum_{i \in \mathcal{A}(w)}\overrightarrow{w}(i))
   :label: concept_vector

5. 文脈類似概念対の獲得
"""""""""""""""""""""""

以上の処理より，全入力概念について概念ベクトルを得ることができる．概念ベクトル間の内積は，概念間の文脈類似度となる．関係構築モジュールでは，文脈類似度に対してある一定の閾値をユーザは設定することができる．ユーザが指定した閾値を越える値を持つ概念対を文脈類似概念対として獲得する．
概念ベクトル :math:`\overrightarrow{C_1}` ， :math:`\overrightarrow{C_2}` ，間の文脈類似度 :math:`sim(\overrightarrow{C_1}, \overrightarrow{C_2})` は， :eq:`context_similarity` を用いて計算する．


.. math:: sim(\overrightarrow{C_1}, \overrightarrow{C_2}) = \frac{\sum_i c_{1,i}c_{2,i}}{\sqrt{\sum_i {c_{1,i}}^2 \sum_i {c_{2,i}}^2}}
   :label: context_similarity

概念間の関係を明示する概念関係子は推定されていないため，推定前の初期値として概念関係子 **non-TAXONOMY** を割当てる．獲得された文脈類似概念対の中には，階層関係が含まれる可能性がある．そのため，概念階層において既に定義されている階層関係については，文脈類似概念対集合の中から除外する．

相関ルールによる概念対の抽出
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

専門文書からその他の関係定義の候補となる概念対を獲得するもう一つの方法として，相関ルールを利用する．相関とは，ある事象が発生すると別の事象が発生しやすいという共起性を意味する．また， :math:`A \Rightarrow B` という相関ルールは， :math:`A` という事象が起こると :math:`B` という事象も起こりやすいことを意味する．相関ルールの抽出は代表的なデータマイニング技術の一つであり，その他の関係定義にも利用されている [Agrawal94]_ ．ここでは，入力文書内の1 文中に同時に出現する入力語の組み合わせを相関ルールとして抽出し，その他の関係定義の候補となる概念対として利用する．抽出された相関ルールに含まれる概念間に，何らかの概念関係が存在すると仮定する．

以下では，相関ルールの定義および相関ルール抽出アルゴリズムApriori について述べる．相関ルールおよびApriori アルゴリズムの説明は，データマイニングの基礎 [Motoda06]_ 2.5節を参考にした．

相関ルールの定義
""""""""""""""""

相関ルールは， :eq:`transaction_set` に示す **トランザクション集合(transaction set)** :math:`T` から抽出される． **トランザクション(transaction)** :math:`t_i` は，データベース内でのデータのまとまりの単位を表す．ここでは，入力文書内の1 文をデータのまとまりの単位としているため，トランザクション集合の要素数 :math:`n` は，入力文書に含まれる文の数を表す．

.. math:: T := \{t_i \mid i=1 \ldots n\}
   :label: transaction_set


:math:`T` の要素 :math:`t_i` は，アイテム集合(item set) である．ここでは，アイテムは入力語とする．つまり， :math:`t_i` は，入力文書の :math:`i` 番目の文に含まれる入力語の集合として表される． :math:`t_i` は， :eq:`transaction` で表される． :eq:`transaction` の :math:`C` は，入力文書に含まれる全入力語の集合を表す．


.. math:: t_i=\{a_{i,j} \mid j = 1 \ldots m, a_{i,j} \in C\}
   :label: transaction

:math:`k` 個のアイテムを含むアイテム集合 :math:`X_k` と :math:`Y_k` について，相関ルールは，:math:`X_k \Rightarrow Y_k (X_k,Y_k \subset C, X_k \cap Y_k = \emptyset)` で表される．ここで，:math:`X_k` を条件部， :math:`Y_k` を結論部と呼ぶ．条件部，結論部共に複数アイテムを含んでいてもよい．

相関ルールの重要性を測る指標として， **支持度** (support) と **確信度** (confidence) がある．支持度とは，相関ルールが全トランザクションでどの程度出現するかを表す割合である．:math:`X_k \Rightarrow Y_k` の支持度 :math:`support(X_k \Rightarrow Y_k)` は，の中でとを共に含むトランザクションの割合により定義される :eq:`support` ．

.. math:: support(X_k \Rightarrow Y_k) = \frac{\mid \{t_i \mid X_k \cup Y_k \subseteq t_i \} \mid}{n}
   :label: support

確信度とは，条件部が起こったときに結論部が起こる割合である． :math:`X_k \Rightarrow Y_k` の確信度 :math:`confidence(X_k \Rightarrow Y_k)` は， :math:`T` において :math:`X_k` を含むトランザクションの中で， :math:`Y_k` が出現する割合により定義される :eq:`confidence` ．

.. math:: confidence(X_k \Rightarrow Y_k) = \frac{\mid \{t_i \mid X_k \cup Y_k \subseteq t_i \} \mid}{\mid \{t_i \mid X_k \subseteq t_i\} \mid}
   :label: confidence

相関ルールの抽出では，支持度と確信度にある一定の閾値を設けないと，組み合わせ爆発を起こし，多数の無意味なルールが生成されてしまう．そのため，相関ルールの抽出では，支持度と確信度に閾値を設け，その値以上の支持度と確信度を有する相関ルールのみを抽出する．ここで，それぞれの閾値を **最小支持度** (minimum support)， **最小確信度** (minimum confidence) と呼ぶ．また，ユーザから与えられた最小支持度以上の支持度を有するアイテム集合を **多頻度アイテム集合** (frequent item set) と呼ぶ．

通常，相関ルールの条件部には複数のアイテムを許すが，ここでは概念対を抽出したいため，条件部と結論部共に一つずつのアイテム，つまり入力語の対を獲得する．WordSpaceを用いた概念対の抽出と同様に，概念間の関係を明示する概念関係子は推定されていないため，初期値として概念関係子 **non-TAXONOMY** を割当てる．

相関ルール抽出アルゴリズム Apriori
""""""""""""""""""""""""""""""""""

相関ルールは，次の二つのステップにより抽出される．

**ステップ1:** 多頻度アイテム集合を獲得する．
**ステップ2:**  から最小確信度以上の確信度を有する相関ルールを導出する．

ステップ2 は，ステップ1 により求めた :math:`F` からルールを導出する処理であり，その負荷は比較的小さい．一方，ステップ1 は， :math:`T` を繰り返し検索し，数多くのアイテム集合の支持度を調べるため，その負荷は大きい．そのため，ステップ1 の効率の良いアルゴリズムを開発することが，実用的な相関ルール抽出アルゴリズムにつながると考えられてきた．この課題をはじめて解決した方法が，IBM アルマデン研究所のRakesh Agrawal らによって提案されたApriori アルゴリズム [Agrawal94]_ である．Apriori アルゴリズムは，現在最も広く利用されている相関ルール抽出アルゴリズムであり，本研究でも関係構築モジュールの実装に用いている．

以下では，Apriori アルゴリズムについて説明する．

Apriori アルゴリズムでは，「 :math:`A` が多頻度アイテム集合であれば，その部分集合は多頻度アイテム集合である」および，その対偶をとって「 :math:`B` が多頻度アイテム集合でなければ， :math:`B` を含むような集合 :math:`A` も多頻度アイテム集合でない」というアイテム集合の支持度の逆単調性を利用している．これらの性質を利用することにより，効率よく枝刈りを実行して，多頻度アイテム集合を求めることができる．例えば，{1,2}が多頻度アイテム集合でなければ，{1,2}を含むいかなるアイテム集合（{1,2,3}など）も多頻度アイテム集合ではないため，その支持度を調べる必要はない．

Apriori アルゴリズムでは，要素数の少ないアイテム集合から支持度を計算し，あるアイテム集合の支持度が最小支持度より小さくなったとき，この逆単調性を利用して，そのアイテム集合を含むようなアイテム集合は，多頻度アイテム集合の候補とはせずに枝狩りする．

要素数 :math:`k` の多頻度アイテム集合を :math:`F_k` ，多頻度アイテム集合の候補集合を :math:`C_k` とする時，Apriori アルゴリズムの処理手順は以下のようになる．

1. :math:`F_k` から :math:`C_{k+1}` を作成する．この際に，:math:`C_{k+1}` の各要素について，要素数 :math:`k` のアイテム集合からなる各部分集合がすべて :math:`F_k` に含まれるかどうかを点検し，そうでなければその要素を :math:`C_{k+1}` から削除する．
2. :math:`T` を検索し， :math:`C_{k+1}` における各要素の支持度を求める．
3. :math:`C_{k+1}` から :math:`F_{k+1}` を抽出する．
4. 新たな多頻度アイテム集合が空となるまで，(1) から(3) の処理を繰り返す．

:numref:`apriori` に，最小支持度0.50 (2/4 = 0.50) における，Apriori アルゴリズムによる多頻度アイテム集合抽出の例を示す． :numref:`apriori` では， :math:`T` には四つのトランザクションが含まれているため， :math:`T` の中で2 回以上出現するアイテム集合が，多頻度アイテム集合となる．はじめに :math:`T` ，から要素数1 のアイテム集合がトランザクションに含まれる回数を数え上げ， :math:`C_1` を作成する．:math:`C_1` の中から最小支持度以上の支持度を有するアイテム集合を抽出し， :math:`F_1` を求める．次に， :math:`F_1` から :math:`C_2` を作成する．ここでは， :math:`C_2` の各要素について，要素数1 のアイテム集合からなる各部分集合は，すべて多頻度アイテム集合となるため，要素の削除は行われない． :math:`T` を検索し， :math:`C_2` から :math:`F_2` を求める．次に， :math:`F_2` から :math:`C_3` を作成する．ここで， :math:`F_2` からは，{1,2,3}および{1,3,5}といったアイテム集合も :math:`C_3` の候補として抽出される．しかし，これらの部分集合である{1,2}および{1,5}は，それぞれ多頻度アイテム集合ではないため，{1,2,3}および{1,3,5}も多頻度アイテム集合ではないことがわかり， :math:`C_3` から削除される．よって， :math:`C_3` は{2,3,5}のみとなる． :math:`T` を検索すると，{2,3,5}の出現数が2であり，支持度は0.50 以上となる．よって， :math:`F_3` は{2,3,5}となる．{2,3,5}からは， :math:`C_4` を作成することができないため，ここで停止することとなる．

.. _apriori:
.. figure:: figures/apriori.png
   :scale: 80 %
   :alt: Apriori アルゴリズムによる多頻度アイテム集合抽出の例
   :align: center

   Apriori アルゴリズムによる多頻度アイテム集合抽出の例


EDR概念記述辞書を用いたプロパティ階層の構築およびその他の関係定義
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

オントロジー構築モジュールは，EDR 概念記述辞書を用いてプロパティ階層の構築およびその他の関係定義を行うことができる．EDR 概念記述辞書には動詞的概念が名詞的概念を支配する場合の格関係を中心に，agent，object， goal， implement，a-object，place， scene， cause の8 種類の概念関係が定義されている．オントロジー構築モジュールはEDR 概念記述辞書に定義されている動詞的概念およびその下位概念をOWLにおけるオブジェクトプロパティとみなし，階層構築時に名詞的概念階層（クラス階層）とは分離してプロパティ階層構築を行う．

また，オントロジー構築モジュールは，8 種類の概念関係のうちagent 関係がある名詞的概念をプロパティの定義域，object 関係がある名詞的概念をプロパティの値域として定義する．

プロパティ階層構築にも，クラス階層構築における完全および部分照合概念階層化と同様のアルゴリズムが適用可能である．完全照合概念を階層化する際には，不要概念の剪定が行われる．そのため，以下の場合にその他の関係定義の整合性が保持できなかったり，その他の関係定義が欠落してしまう問題が発生する．

1. クラス階層中の剪定された概念がagent またはobject の値として定義されている場合
2. プロパティ階層中の剪定された概念にagent またはobject 関係が定義されている場合

オントロジー構築モジュールでは，1. については，agent またはobject の値を，剪定された概念の下位概念に置換することで整合性を保持している．2. については，剪定されたプロパティの下位概念に定義域および値域を継承させることによりその他の関係定義が欠落しないようにしている．

参考文献
--------
.. [Hearst96] M.A. Hearst, and H. Sch¨utze, “Customizing a Lexicon to Better Suit a Computational Task,” Corpus Processing for Lexical Acquisition, pp.77–96, MIT Press, 1996.
.. [Agrawal94] R. Agrawal, and R. Srikant, “Fast Algorithms for Mining Association Rules in Large Databases,” Processing of the 20th International Conference Very Large Data Bases, VLDB, pp.487.499, Morgan Kaufmann, 1994.
.. [Motoda06] 元田浩，津本周作，山口高平，沼尾政行，データマイニングの基礎，オーム社，2006.

